{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Name:__ Tonson Praphabkul  \n",
    "__Student id:__ st123010"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lab report\n",
    "&emsp; In this lab, I experiments with 4 models, which is \"AlexNet with LRN\", \"Alexnet without LRN\", \"GoogleNet\" and \"Pre-trained GoogleNet\" from torchvision. The dataset that I used to experiments in this lab is CIFAR-10. I has transformed all of the input data with the code below.  \n",
    "  \n",
    "preprocess = transforms.Compose([  \n",
    "&emsp;transforms.Resize(256),  \n",
    "&emsp;transforms.CenterCrop(224),  \n",
    "&emsp;transforms.ToTensor(),  \n",
    "&emsp;transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010))])  \n",
    "\n",
    "&emsp; The input data got seperated into train set (40,000), validation set (10,000), test set (10,000) roughly 66%, 16%, 16% respectively. The batch size for each dataloaders is 4 and only shuffle the train set. The criterion for the model is CrossEntropyLoss and optimizer is Stochastic gradient descent with learning rate of 0.0001 and momentum of 0.9. I trained those model for 10 epochs each. The results is display at the table below.\n",
    "\n",
    "\n",
    "|         Model        | Train Accuracy | Validation accruacy | Test accruacy | Number of parameters | Time used |\n",
    "|:--------------------:|----------------|:-------------------:|:-------------:|----------------------|-----------|\n",
    "| AlexNet with LRN     | 0.61           | 0.63                | 0.63          | 58,322,314           | 30m 44s   |\n",
    "| Alexnet without LRN  | 0.72           | 0.73                | 0.73          | 58,322,314           | 31m 25s   |\n",
    "| GoogleNet            | 0.83           | 0.82                | 0.82          | 10,635,134           | 114m 10s  |\n",
    "| Pretrained_GoogleNet | 0.96           | 0.94                | 0.93          | 13,004,888           | 91m 25s   |\n",
    "\n",
    "\n",
    "&emsp; There are few interesting things that I liked to point out. First, AlexNet with LRN is performs worse than AlexNet without LRN. My assumption is in this case some important information got loss when LRN adjusts/contrast the brightness. Second, GoogleNet takes way more time to train than AlexNet even with far less number of parameters. I think it come from the complexity of neural network in term of height. GoogleNet models is far more deeper than Alexnet.\n",
    "  \n",
    "&emsp; The pretrain GoogleNet from torchvision performs the best, far surpass other model. The number of parameters of pretrained GoogleNet is higher than the GoogleNet that I usded (modify from the lab provided code). All of the models seem to have no problems with generalization and I think if I increases the number of epoch, we can still increase the accruacy. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Alexnet_Wo_LRN(nn.Module):\n",
    "    \n",
    "    def __init__(self, output_size):\n",
    "        super().__init__()  #inherit everything from nn.Module\n",
    "        self.layer1 = nn.Conv2d(3, 96, kernel_size=11, stride=4, padding=2)\n",
    "        self.layer2 = nn.Conv2d(96, 256, kernel_size=5, padding=2)\n",
    "        self.layer3 = nn.Conv2d(256, 384, kernel_size=3, padding=1)\n",
    "        self.layer4 = nn.Conv2d(384, 384, kernel_size=3, padding=1)\n",
    "        self.layer5 = nn.Conv2d(384, 256, kernel_size=3, padding=1)\n",
    "        self.layer6 = nn.Linear(256 * 6 * 6, 4096)\n",
    "        self.layer7 = nn.Linear(4096, 4096)\n",
    "        self.layer8 = nn.Linear(4096, output_size)\n",
    "        self.Adapavgpool = nn.AdaptiveAvgPool2d((6, 6))\n",
    "        self.relu   = nn.ReLU(inplace=True)\n",
    "        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2)\n",
    "        self.dropout = nn.Dropout()\n",
    "\n",
    "    def flatten(self, input_flatten):\n",
    "        batch_size = input_flatten.shape[0]\n",
    "        return input_flatten.view(batch_size, -1)\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        out = self.layer1(x)\n",
    "        out = self.relu(out)\n",
    "        out = self.maxpool(out)\n",
    "\n",
    "        out = self.layer2(out)\n",
    "        out = self.relu(out)\n",
    "        out = self.maxpool(out)\n",
    "\n",
    "        out = self.layer3(out)\n",
    "        out = self.relu(out)\n",
    "\n",
    "        out = self.layer4(out)\n",
    "        out = self.relu(out)\n",
    "\n",
    "        out = self.layer5(out)\n",
    "        out = self.relu(out)\n",
    "        out = self.maxpool(out)\n",
    "\n",
    "        out = self.Adapavgpool(out)\n",
    "        out = self.flatten(out)\n",
    "        out = self.dropout(out)\n",
    "\n",
    "        out = self.layer6(out)\n",
    "        out = self.relu(out)\n",
    "\n",
    "        out = self.dropout(out)\n",
    "        out = self.layer7(out)\n",
    "        out = self.relu(out)\n",
    "\n",
    "        out = self.layer8(out)\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Alexnet_LRN(nn.Module):\n",
    "    \n",
    "    def __init__(self, output_size):\n",
    "        super().__init__()  #inherit everything from nn.Module\n",
    "        self.layer1 = nn.Conv2d(3, 96, kernel_size=11, stride=4, padding=2)\n",
    "        self.layer2 = nn.Conv2d(96, 256, kernel_size=5, padding=2)\n",
    "        self.layer3 = nn.Conv2d(256, 384, kernel_size=3, padding=1)\n",
    "        self.layer4 = nn.Conv2d(384, 384, kernel_size=3, padding=1)\n",
    "        self.layer5 = nn.Conv2d(384, 256, kernel_size=3, padding=1)\n",
    "        self.layer6 = nn.Linear(256 * 6 * 6, 4096)\n",
    "        self.layer7 = nn.Linear(4096, 4096)\n",
    "        self.layer8 = nn.Linear(4096, output_size)\n",
    "        self.Adapavgpool = nn.AdaptiveAvgPool2d((6, 6))\n",
    "        self.relu   = nn.ReLU(inplace=True)\n",
    "        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2)\n",
    "        self.dropout = nn.Dropout()\n",
    "        self.lrn     = nn.LocalResponseNorm(5, alpha=0.00001, beta = 0.75, k = 2.0)\n",
    "\n",
    "    def flatten(self, input_flatten):\n",
    "        batch_size = input_flatten.shape[0]\n",
    "        return input_flatten.view(batch_size, -1)\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        out = self.layer1(x)\n",
    "        out = self.relu(out)\n",
    "        out = self.lrn(out)\n",
    "        out = self.maxpool(out)\n",
    "\n",
    "        out = self.layer2(out)\n",
    "        out = self.relu(out)\n",
    "        out = self.lrn(out)\n",
    "        out = self.maxpool(out)\n",
    "\n",
    "        out = self.layer3(out)\n",
    "        out = self.relu(out)\n",
    "\n",
    "        out = self.layer4(out)\n",
    "        out = self.relu(out)\n",
    "\n",
    "        out = self.layer5(out)\n",
    "        out = self.relu(out)\n",
    "        out = self.maxpool(out)\n",
    "\n",
    "        out = self.Adapavgpool(out)\n",
    "        out = self.flatten(out)\n",
    "        out = self.dropout(out)\n",
    "\n",
    "        out = self.layer6(out)\n",
    "        out = self.relu(out)\n",
    "\n",
    "        out = self.dropout(out)\n",
    "        out = self.layer7(out)\n",
    "        out = self.relu(out)\n",
    "\n",
    "        out = self.layer8(out)\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Inception(nn.Module):\n",
    "    '''\n",
    "    Inception block for a GoogLeNet-like CNN\n",
    "\n",
    "    Attributes\n",
    "    ----------\n",
    "    in_planes : int\n",
    "        Number of input feature maps\n",
    "    n1x1 : int\n",
    "        Number of direct 1x1 convolutions\n",
    "    n3x3red : int\n",
    "        Number of 1x1 reductions before the 3x3 convolutions\n",
    "    n3x3 : int\n",
    "        Number of 3x3 convolutions\n",
    "    n5x5red : int\n",
    "        Number of 1x1 reductions before the 5x5 convolutions\n",
    "    n5x5 : int\n",
    "        Number of 5x5 convolutions\n",
    "    pool_planes : int\n",
    "        Number of 1x1 convolutions after 3x3 max pooling\n",
    "    b1 : Sequential\n",
    "        First branch (direct 1x1 convolutions)\n",
    "    b2 : Sequential\n",
    "        Second branch (reduction then 3x3 convolutions)\n",
    "    b3 : Sequential\n",
    "        Third branch (reduction then 5x5 convolutions)\n",
    "    b4 : Sequential\n",
    "        Fourth branch (max pooling then reduction)\n",
    "    '''\n",
    "    \n",
    "    def __init__(self, in_planes, n1x1, n3x3red, n3x3, n5x5red, n5x5, pool_planes):\n",
    "        super(Inception, self).__init__()\n",
    "        self.in_planes = in_planes\n",
    "        self.n1x1 = n1x1\n",
    "        self.n3x3red = n3x3red\n",
    "        self.n3x3 = n3x3\n",
    "        self.n5x5red = n5x5red\n",
    "        self.n5x5 = n5x5\n",
    "        self.pool_planes = pool_planes\n",
    "        \n",
    "        # 1x1 conv branch\n",
    "        self.b1 = nn.Sequential(\n",
    "            nn.Conv2d(in_planes, n1x1, kernel_size=1),\n",
    "            nn.BatchNorm2d(n1x1),\n",
    "            nn.ReLU(True),\n",
    "        )\n",
    "\n",
    "        # 1x1 conv -> 3x3 conv branch\n",
    "        self.b2 = nn.Sequential(\n",
    "            nn.Conv2d(in_planes, n3x3red, kernel_size=1),\n",
    "            nn.BatchNorm2d(n3x3red),\n",
    "            nn.ReLU(True),\n",
    "            nn.Conv2d(n3x3red, n3x3, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(n3x3),\n",
    "            nn.ReLU(True),\n",
    "        )\n",
    "\n",
    "        # 1x1 conv -> 5x5 conv branch\n",
    "        self.b3 = nn.Sequential(\n",
    "            nn.Conv2d(in_planes, n5x5red, kernel_size=1),\n",
    "            nn.BatchNorm2d(n5x5red),\n",
    "            nn.ReLU(True),\n",
    "            nn.Conv2d(n5x5red, n5x5, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(n5x5),\n",
    "            nn.ReLU(True),\n",
    "            nn.Conv2d(n5x5, n5x5, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(n5x5),\n",
    "            nn.ReLU(True),\n",
    "        )\n",
    "\n",
    "        # 3x3 pool -> 1x1 conv branch\n",
    "        self.b4 = nn.Sequential(\n",
    "            nn.MaxPool2d(3, stride=1, padding=1),\n",
    "            nn.Conv2d(in_planes, pool_planes, kernel_size=1),\n",
    "            nn.BatchNorm2d(pool_planes),\n",
    "            nn.ReLU(True),\n",
    "        )\n",
    "            \n",
    "\n",
    "    def forward(self, x):\n",
    "        y1 = self.b1(x)\n",
    "        y2 = self.b2(x)\n",
    "        y3 = self.b3(x)\n",
    "        y4 = self.b4(x)\n",
    "        return torch.cat([y1, y2, y3, y4], 1) # <<<< dim 1 = channel\n",
    "\n",
    "class Aux_layer(nn.Module):\n",
    "    def __init__(self, position):    \n",
    "        super(Aux_layer, self).__init__()\n",
    "        \n",
    "        self.avgpool = nn.AvgPool2d(5, stride=3) \n",
    "        self.position = position\n",
    "        \n",
    "        if self.position == 'a4':\n",
    "            self.conv = nn.Sequential(nn.Conv2d(512,128,kernel_size = 1,stride = 1), nn.ReLU(True))   \n",
    "        else:\n",
    "            self.conv = nn.Sequential(nn.Conv2d(528,128,kernel_size = 1,stride = 1), nn.ReLU(True))   \n",
    "\n",
    "        self.fc1 = nn.Sequential(nn.Linear(2048, 1024), nn.ReLU(True), nn.Dropout(0.7))\n",
    "        self.fc2 = nn.Linear(1024, 10)\n",
    "    \n",
    "    def forward(self,x):\n",
    "        aux = self.avgpool(x)\n",
    "        aux = self.conv(aux)\n",
    "#         print(f'aux conv: {aux.shape}')\n",
    "        aux = aux.flatten(start_dim = 1)\n",
    "#         print(f'aux: {aux.shape}')\n",
    "        aux = self.fc1(aux)\n",
    "        aux = self.fc2(aux)\n",
    "        return aux\n",
    "\n",
    "class GoogLeNet(nn.Module):\n",
    "    '''\n",
    "    GoogLeNet-like CNN\n",
    "\n",
    "    Attributes\n",
    "    ----------\n",
    "    pre_layers : Sequential\n",
    "        Initial convolutional layer\n",
    "    a3 : Inception\n",
    "        First inception block\n",
    "    b3 : Inception\n",
    "        Second inception block\n",
    "    maxpool : MaxPool2d\n",
    "        Pooling layer after second inception block\n",
    "    a4 : Inception\n",
    "        Third inception block\n",
    "    b4 : Inception\n",
    "        Fourth inception block\n",
    "    c4 : Inception\n",
    "        Fifth inception block\n",
    "    d4 : Inception\n",
    "        Sixth inception block\n",
    "    e4 : Inception\n",
    "        Seventh inception block\n",
    "    a5 : Inception\n",
    "        Eighth inception block\n",
    "    b5 : Inception\n",
    "        Ninth inception block\n",
    "    avgpool : AvgPool2d\n",
    "        Average pool layer after final inception block\n",
    "    linear : Linear\n",
    "        Fully connected layer\n",
    "    '''\n",
    "\n",
    "    def __init__(self):\n",
    "        super(GoogLeNet, self).__init__()\n",
    "        \n",
    "        self.is_debug = False\n",
    "        \n",
    "        self.pre_layers = nn.Sequential(\n",
    "            nn.Conv2d(3,64, kernel_size=7, stride =2, padding =3),\n",
    "            nn.ReLU(True),\n",
    "            nn.MaxPool2d(3, stride =2, padding = 1),\n",
    "            nn.LocalResponseNorm(5, alpha = 0.0001, beta = 0.75, k = 2.0),\n",
    "            nn.Conv2d(64, 64, kernel_size=1),\n",
    "            nn.ReLU(True),\n",
    "            nn.Conv2d(64, 192, kernel_size=3, stride = 1, padding=1),\n",
    "            nn.ReLU(True),\n",
    "            nn.LocalResponseNorm(5, alpha = 0.0001, beta = 0.75, k = 2.0),              \n",
    "            nn.MaxPool2d(3, stride =2, padding =1),\n",
    "        )\n",
    "\n",
    "        self.a3 = Inception(192,  64,  96, 128, 16, 32, 32)\n",
    "        self.b3 = Inception(256, 128, 128, 192, 32, 96, 64)\n",
    "\n",
    "        self.maxpool = nn.MaxPool2d(3, stride=2, padding=1)\n",
    "\n",
    "        self.a4 = Inception(480, 192,  96, 208, 16,  48,  64)\n",
    "        self.b4 = Inception(512, 160, 112, 224, 24,  64,  64)\n",
    "        self.c4 = Inception(512, 128, 128, 256, 24,  64,  64)\n",
    "        self.d4 = Inception(512, 112, 144, 288, 32,  64,  64)\n",
    "        self.e4 = Inception(528, 256, 160, 320, 32, 128, 128)\n",
    "\n",
    "        self.a5 = Inception(832, 256, 160, 320, 32, 128, 128)\n",
    "        self.b5 = Inception(832, 384, 192, 384, 48, 128, 128)\n",
    "\n",
    "        self.avgpool= nn.AvgPool2d(7, stride=1) ## orig = 8\n",
    "        self.dropout = nn.Dropout(0.4)\n",
    "        self.linear = nn.Linear(1024, 10)\n",
    "        \n",
    "        self.aux_a4 = Aux_layer('a4')\n",
    "        self.aux_d4 = Aux_layer('d4')\n",
    "    \n",
    "    def forward(self, x):\n",
    "        out = self.pre_layers(x)\n",
    "        \n",
    "        if self.is_debug : print(f'pre-layer: {out.shape}')\n",
    "            \n",
    "        out = self.a3(out)\n",
    "        if self.is_debug : print(f'a3: {out.shape}')\n",
    "            \n",
    "        out = self.b3(out)\n",
    "        if self.is_debug : print(f'b3 :{out.shape}')\n",
    "            \n",
    "        out = self.maxpool(out)\n",
    "        if self.is_debug : print(f'maxpool: {out.shape}')\n",
    "            \n",
    "        out = self.a4(out)\n",
    "        if self.is_debug : print(f'a4: {out.shape}')\n",
    "            \n",
    "        aux_a4 = self.aux_a4(out)\n",
    "        out = self.b4(out)\n",
    "        if self.is_debug : print(f'b4 : {out.shape}')\n",
    "            \n",
    "        out = self.c4(out)\n",
    "        if self.is_debug : print(f'c4 : {out.shape}') \n",
    "            \n",
    "        out = self.d4(out)\n",
    "        if self.is_debug : print(f'd4 : {out.shape}')\n",
    "            \n",
    "        aux_d4 = self.aux_d4(out)\n",
    "        out = self.e4(out)\n",
    "        if self.is_debug : print(f'e4 : {out.shape}')\n",
    "            \n",
    "        out = self.maxpool(out)\n",
    "        if self.is_debug : print(f'maxpool : {out.shape}')\n",
    "            \n",
    "        out = self.a5(out)\n",
    "        if self.is_debug : print(f'a5 : {out.shape}')\n",
    "            \n",
    "        out = self.b5(out)\n",
    "        if self.is_debug : print(f'b5 : {out.shape}')\n",
    "            \n",
    "        out = self.avgpool(out)\n",
    "        if self.is_debug : print(f'avgpool : {out.shape}')\n",
    "            \n",
    "        out = self.dropout(out)\n",
    "        out = out.view(out.size(0), -1)\n",
    "        out = self.linear(out)\n",
    "        \n",
    "        if self.training == True:\n",
    "            return out, aux_a4, aux_d4\n",
    "        else:\n",
    "            return out"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.6 (main, Nov  2 2022, 18:53:38) [GCC 11.3.0]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
